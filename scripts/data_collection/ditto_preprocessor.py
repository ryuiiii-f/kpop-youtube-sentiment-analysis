import pandas as pd
import re
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥è¯­è¨€æ£€æµ‹åº“ï¼ˆå¦‚æœæ²¡æœ‰è¯·pip install langdetectï¼‰
try:
    from langdetect import detect, DetectorFactory
    DetectorFactory.seed = 0  # ç¡®ä¿ç»“æœä¸€è‡´æ€§
    LANG_DETECT_AVAILABLE = True
except ImportError:
    print("âš ï¸ langdetect not installed. Language detection will be skipped.")
    print("Install with: pip install langdetect")
    LANG_DETECT_AVAILABLE = False

# ğŸ¯ åŠ è½½Dittoè¯„è®ºæ•°æ®
input_file = "newjeans_ditto_ditto_side_a_comments.csv"  # æ ¹æ®ä½ çš„æ–‡ä»¶åè°ƒæ•´
df = pd.read_csv(input_file)

print(f"ğŸ“Š Original dataset: {len(df)} comments")
print(f"ğŸ“ Source file: {input_file}")
print("-" * 50)

def detect_language(text):
    """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
    if not LANG_DETECT_AVAILABLE:
        return "unknown"
    
    try:
        # å»æ‰è¿‡å¤šçš„è¡¨æƒ…ç¬¦å·å’Œç¬¦å·
        clean_text = re.sub(r'[^\w\sê°€-í£ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠä¸€-é¾¯]', ' ', str(text))
        if len(clean_text.strip()) < 3:
            return "emoji_only"
        return detect(clean_text)
    except:
        return "unknown"

def classify_comment_type(text):
    """åŸºäºå†…å®¹åˆ†ç±»è¯„è®ºç±»å‹ï¼ˆé’ˆå¯¹Dittoç‰¹è‰²ï¼‰"""
    text_lower = str(text).lower()
    
    # MVè§£è¯»å…³é”®è¯
    mv_keywords = [
        'side a', 'side b', 'heesoo', 'theory', 'meaning', 'explain', 
        'story', 'mv', 'video', 'concept', 'interpretation', 'analysis',
        'ì´ë¡ ', 'ì˜ë¯¸', 'ìŠ¤í† ë¦¬', 'í•´ì„', 'ë¶„ì„'
    ]
    
    # æƒ…æ„Ÿå…±é¸£å…³é”®è¯  
    emotion_keywords = [
        'nostalgia', 'winter', 'memory', 'feeling', 'mood', 'vibe',
        'relate', 'sad', 'beautiful', 'emotional', 'tears',
        'ê°ì •', 'ê¸°ì–µ', 'ê²¨ìš¸', 'ì¶”ì–µ', 'ëŠë‚Œ'
    ]
    
    # éŸ³ä¹æŠ€æœ¯å…³é”®è¯
    music_keywords = [
        'vocal', 'voice', 'harmony', 'production', 'beat', 'sound',
        'mixing', 'melody', 'instrumental', 'choreography',
        'ë³´ì»¬', 'ëª©ì†Œë¦¬', 'ìŒì•…', 'ì•ˆë¬´'
    ]
    
    # çº¯åº”æ´å…³é”®è¯
    stan_keywords = [
        'queen', 'talent', 'visual', 'love', 'stan', 'amazing', 
        'perfect', 'best', 'bias', 'ot5',
        'ìµœê³ ', 'ì‚¬ë‘', 'ì™„ë²½', 'ì—¬ì™•'
    ]
    
    if any(keyword in text_lower for keyword in mv_keywords):
        return "mv_analysis"
    elif any(keyword in text_lower for keyword in emotion_keywords):
        return "emotional_response" 
    elif any(keyword in text_lower for keyword in music_keywords):
        return "music_technical"
    elif any(keyword in text_lower for keyword in stan_keywords):
        return "fan_support"
    else:
        return "general"

def is_valid_ditto_comment(text):
    """æ”¹è¿›çš„è¯„è®ºæœ‰æ•ˆæ€§æ£€æŸ¥ï¼ˆé€‚é…å¤šè¯­è¨€ï¼‰"""
    if pd.isna(text):
        return False
    
    text = str(text).strip()
    
    # è¿‡çŸ­çš„è¯„è®º
    if len(text) < 2:
        return False
    
    # çº¯è¡¨æƒ…ç¬¦å·æˆ–ç¬¦å·çš„è¯„è®º
    clean_text = re.sub(r'[^\w\sê°€-í£ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠä¸€-é¾¯]', '', text)
    if len(clean_text.strip()) < 2:
        return False
        
    # è¿‡æ»¤æ˜æ˜¾çš„åƒåœ¾è¯„è®º
    spam_patterns = [
        r'^.{1,3}$',  # è¿‡çŸ­
        r'^[!@#$%^&*()_+={}[\]|\\:";\'<>?,.\/~`\-]*$',  # çº¯ç¬¦å·
        r'^(\w)\1{10,}',  # é‡å¤å­—ç¬¦
    ]
    
    for pattern in spam_patterns:
        if re.match(pattern, text):
            return False
    
    return True

def advanced_deduplication(df):
    """é«˜çº§å»é‡ï¼šè€ƒè™‘ç›¸ä¼¼è¯„è®ºå’Œå¤šè¯­è¨€"""
    
    # 1. å®Œå…¨é‡å¤å»é‡
    original_len = len(df)
    df = df.drop_duplicates(subset=["comment_text"])
    print(f"ğŸ”„ Exact duplicates removed: {original_len - len(df)}")
    
    # 2. æ ‡å‡†åŒ–åå»é‡ï¼ˆå»é™¤ç©ºæ ¼ã€æ ‡ç‚¹å·®å¼‚ï¼‰
    def normalize_text(text):
        # ç§»é™¤å¤šä½™ç©ºæ ¼ã€æ ‡ç‚¹ï¼Œè½¬å°å†™
        normalized = re.sub(r'[^\w\sê°€-í£ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠä¸€-é¾¯]', '', str(text).lower())
        return ' '.join(normalized.split())
    
    df['normalized_text'] = df['comment_text'].apply(normalize_text)
    before_norm = len(df)
    df = df.drop_duplicates(subset=['normalized_text'])
    print(f"ğŸ”„ Normalized duplicates removed: {before_norm - len(df)}")
    
    # æ¸…ç†ä¸´æ—¶åˆ—
    df = df.drop('normalized_text', axis=1)
    
    return df

# ğŸ§¹ æ•°æ®æ¸…æ´—æµç¨‹
print("ğŸ§¹ Starting data cleaning...")

# 1. åŸºç¡€æ¸…æ´—
df["comment_text"] = df["comment_text"].astype(str)
original_count = len(df)
df = df[df["comment_text"].apply(is_valid_ditto_comment)]
print(f"ğŸ“ Valid comments: {len(df)} (removed {original_count - len(df)} invalid)")

# 2. é«˜çº§å»é‡
df = advanced_deduplication(df)

# 3. è¯­è¨€æ£€æµ‹
if LANG_DETECT_AVAILABLE:
    print("ğŸŒ Detecting languages...")
    df['detected_language'] = df['comment_text'].apply(detect_language)
    
    # è¯­è¨€åˆ†å¸ƒç»Ÿè®¡
    lang_dist = df['detected_language'].value_counts()
    print("ğŸ“Š Language distribution:")
    for lang, count in lang_dist.head(5).items():
        percentage = count/len(df)*100
        print(f"   {lang}: {count} ({percentage:.1f}%)")

# 4. è¯„è®ºç±»å‹åˆ†ç±»
print("ğŸ­ Classifying comment types...")
df['comment_type'] = df['comment_text'].apply(classify_comment_type)

# è¯„è®ºç±»å‹åˆ†å¸ƒ
type_dist = df['comment_type'].value_counts()
print("ğŸ“Š Comment type distribution:")
for ctype, count in type_dist.items():
    percentage = count/len(df)*100
    print(f"   {ctype}: {count} ({percentage:.1f}%)")

# 5. è¯„è®ºé•¿åº¦å’Œè´¨é‡æŒ‡æ ‡
df['comment_length'] = df['comment_text'].str.len()
df['word_count'] = df['comment_text'].apply(lambda x: len(str(x).split()))

# 6. æ—¶é—´å¤„ç†ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³ï¼‰
if 'published_at' in df.columns:
    df['published_at'] = pd.to_datetime(df['published_at'])
    df['hour_of_day'] = df['published_at'].dt.hour
    df['day_of_week'] = df['published_at'].dt.day_name()

# 7. è´¨é‡åˆ†æ•°è®¡ç®—
def calculate_quality_score(row):
    """ç»¼åˆè¯„ä¼°è¯„è®ºè´¨é‡"""
    score = 0
    
    # é•¿åº¦åˆ†æ•° (0-30åˆ†)
    length = row['comment_length']
    if length > 100:
        score += 30
    elif length > 50:
        score += 20
    elif length > 20:
        score += 10
    
    # ç‚¹èµåˆ†æ•° (0-40åˆ†)
    likes = row.get('like_count', 0)
    if likes > 50:
        score += 40
    elif likes > 10:
        score += 30
    elif likes > 5:
        score += 20
    elif likes > 0:
        score += 10
    
    # å›å¤åˆ†æ•° (0-20åˆ†)
    replies = row.get('reply_count', 0)
    if replies > 5:
        score += 20
    elif replies > 0:
        score += 10
    
    # ç±»å‹åˆ†æ•° (0-10åˆ†)
    if row['comment_type'] in ['mv_analysis', 'emotional_response']:
        score += 10
    elif row['comment_type'] == 'music_technical':
        score += 5
    
    return score

df['quality_score'] = df.apply(calculate_quality_score, axis=1)

# ğŸ“Š æ¸…æ´—ç»“æœç»Ÿè®¡
print("\n" + "="*50)
print("ğŸ“ˆ CLEANING SUMMARY")
print("="*50)
print(f"âœ… Final dataset: {len(df)} comments")
print(f"ğŸ“ Average comment length: {df['comment_length'].mean():.1f} characters")
print(f"ğŸ“ Average word count: {df['word_count'].mean():.1f} words")
print(f"ğŸ‘ Average likes: {df.get('like_count', pd.Series([0])).mean():.1f}")
print(f"ğŸ† Average quality score: {df['quality_score'].mean():.1f}/100")

# ä¿å­˜æ¸…æ´—åçš„æ•°æ®
output_file = "clean_comments_ditto_side_a.csv"
df.to_csv(output_file, index=False, encoding="utf-8-sig")
print(f"\nğŸ’¾ Cleaned data saved to: {output_file}")

# 8. é«˜è´¨é‡è¯„è®ºé¢„è§ˆ
print("\nğŸ”¥ TOP 3 HIGHEST QUALITY COMMENTS:")
top_quality = df.nlargest(3, 'quality_score')[['comment_text', 'quality_score', 'comment_type', 'detected_language']]
for i, row in top_quality.iterrows():
    print(f"\nScore: {row['quality_score']}/100 | Type: {row['comment_type']} | Lang: {row.get('detected_language', 'N/A')}")
    print(f"Text: {row['comment_text'][:150]}...")

print("\nğŸ¯ Ready for sentiment analysis and clustering!")
