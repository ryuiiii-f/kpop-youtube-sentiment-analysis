import os
import pandas as pd
from googleapiclient.discovery import build
from dotenv import load_dotenv
from tqdm import tqdm
import time
from datetime import datetime

# âœ… è½½å…¥ API å¯†é’¥ï¼ˆä» .env æ–‡ä»¶ï¼‰
load_dotenv()
API_KEY = os.getenv("YOUTUBE_API_KEY")

# ğŸ¯ NewJeans Ditto è§†é¢‘ä¿¡æ¯
VIDEOS = {
    "ditto_side_a": {
        "id": "pSUydWEqKwE", 
        "title": "NewJeans 'Ditto' Side A"
    },
    "ditto_side_b": {
        "id": "V37TaRdVUQY", 
        "title": "NewJeans 'Ditto' Side B"
    },
    "ditto_performance": {
        "id": "Km71Rr9K-Bw", 
        "title": "NewJeans 'Ditto' Performance"
    }
}

MAX_COMMENTS = 1000  # å»ºè®®å¢åŠ åˆ°1000ï¼ŒDittoè¯„è®ºè´¨é‡é«˜
SELECTED_VIDEO = "ditto_side_a"  # ä¸»è¦åˆ†æSide A

# âœ… åˆå§‹åŒ– YouTube API å®¢æˆ·ç«¯
youtube = build("youtube", "v3", developerKey=API_KEY)

# âœ… å¢å¼ºç‰ˆè¯„è®ºæŠ“å–å‡½æ•°
def get_comments_enhanced(video_id, max_comments, video_title):
    """
    å¢å¼ºç‰ˆè¯„è®ºæ”¶é›†ï¼ŒåŒ…å«æ›´å¤šmetadata
    """
    comments = []
    next_page_token = None
    pbar = tqdm(total=max_comments, desc=f"Fetching {video_title}")

    # å…ˆè·å–è§†é¢‘åŸºç¡€ä¿¡æ¯
    video_info = get_video_stats(video_id)
    
    while len(comments) < max_comments:
        try:
            request = youtube.commentThreads().list(
                part="snippet,replies",  # å¢åŠ repliesè·å–å›å¤æ•°
                videoId=video_id,
                maxResults=100,
                pageToken=next_page_token,
                order="relevance",  # Dittoé€‚åˆç”¨relevanceï¼Œèƒ½è·å–é«˜è´¨é‡è®¨è®º
                textFormat="plainText"
            )
            response = request.execute()

            for item in response["items"]:
                snippet = item["snippet"]["topLevelComment"]["snippet"]
                
                # ğŸ” å¢å¼ºæ•°æ®æ”¶é›†
                comment = {
                    "video_id": video_id,
                    "video_title": video_title,
                    "comment_id": item["snippet"]["topLevelComment"]["id"],
                    "comment_text": snippet["textDisplay"],
                    "author_name": snippet["authorDisplayName"],
                    "like_count": snippet["likeCount"],
                    "reply_count": item["snippet"]["totalReplyCount"],
                    "published_at": snippet["publishedAt"],
                    "updated_at": snippet.get("updatedAt", snippet["publishedAt"]),
                    "comment_length": len(snippet["textDisplay"]),
                    "is_reply": False  # é¡¶çº§è¯„è®ºæ ‡è®°
                }
                
                # è®¡ç®—å‘å¸ƒåçš„å¤©æ•°
                pub_date = datetime.fromisoformat(snippet["publishedAt"].replace('Z', '+00:00'))
                days_since_upload = (datetime.now().astimezone() - pub_date).days
                comment["days_since_upload"] = days_since_upload
                
                comments.append(comment)
                pbar.update(1)
                
                if len(comments) >= max_comments:
                    break
                
                # ğŸ”¥ å¯é€‰ï¼šæ”¶é›†é«˜è´¨é‡å›å¤
                if item["snippet"]["totalReplyCount"] > 5:  # å›å¤å¤šçš„è¯„è®ºå¾€å¾€è®¨è®ºåº¦é«˜
                    replies = get_comment_replies(item["snippet"]["topLevelComment"]["id"], max_replies=3)
                    comments.extend(replies)

            next_page_token = response.get("nextPageToken")
            if not next_page_token:
                break
                
            # APIå‹å¥½å»¶è¿Ÿ
            time.sleep(0.1)
            
        except Exception as e:
            print(f"Error fetching comments: {e}")
            break

    pbar.close()
    
    # æ·»åŠ è§†é¢‘ç»Ÿè®¡ä¿¡æ¯åˆ°æ¯æ¡è¯„è®º
    for comment in comments:
        comment.update(video_info)
    
    return comments

def get_video_stats(video_id):
    """è·å–è§†é¢‘åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
    try:
        request = youtube.videos().list(
            part="statistics,snippet",
            id=video_id
        )
        response = request.execute()
        
        if response["items"]:
            stats = response["items"][0]["statistics"]
            snippet = response["items"][0]["snippet"]
            return {
                "video_view_count": int(stats.get("viewCount", 0)),
                "video_like_count": int(stats.get("likeCount", 0)),
                "video_comment_count": int(stats.get("commentCount", 0)),
                "video_upload_date": snippet["publishedAt"],
                "video_description": snippet["description"][:200] + "..."  # æˆªå–æè¿°
            }
    except Exception as e:
        print(f"Error fetching video stats: {e}")
        return {}

def get_comment_replies(comment_id, max_replies=3):
    """è·å–é«˜è´¨é‡è¯„è®ºçš„å›å¤"""
    replies = []
    try:
        request = youtube.comments().list(
            part="snippet",
            parentId=comment_id,
            maxResults=max_replies,
            textFormat="plainText"
        )
        response = request.execute()
        
        for item in response["items"]:
            snippet = item["snippet"]
            reply = {
                "comment_id": item["id"],
                "parent_id": comment_id,
                "comment_text": snippet["textDisplay"],
                "author_name": snippet["authorDisplayName"],
                "like_count": snippet["likeCount"],
                "reply_count": 0,  # å›å¤çš„å›å¤ä¸è€ƒè™‘
                "published_at": snippet["publishedAt"],
                "comment_length": len(snippet["textDisplay"]),
                "is_reply": True
            }
            replies.append(reply)
    except Exception as e:
        print(f"Error fetching replies: {e}")
    
    return replies

# âœ… æ‰§è¡ŒæŠ“å–å¹¶ä¿å­˜
if __name__ == "__main__":
    print("ğŸµ NewJeans 'Ditto' Comment Analysis Project")
    print(f"Target: {VIDEOS[SELECTED_VIDEO]['title']}")
    print(f"Expected comments: {MAX_COMMENTS}")
    print("-" * 50)
    
    video_info = VIDEOS[SELECTED_VIDEO]
    data = get_comments_enhanced(
        video_info["id"], 
        MAX_COMMENTS, 
        video_info["title"]
    )
    
    # ä¿å­˜ä¸ºCSV
    df = pd.DataFrame(data)
    output_filename = f"newjeans_ditto_{SELECTED_VIDEO}_comments.csv"
    df.to_csv(output_filename, index=False, encoding="utf-8-sig")
    
    # ğŸ“Š å¿«é€Ÿç»Ÿè®¡
    print("\nâœ… Collection Summary:")
    print(f"ğŸ“ Total comments collected: {len(df)}")
    print(f"ğŸ’¬ Average comment length: {df['comment_length'].mean():.1f} chars")
    print(f"ğŸ‘ Average likes per comment: {df['like_count'].mean():.1f}")
    print(f"ğŸ”„ Comments with replies: {len(df[df['reply_count'] > 0])}")
    
    # è¯­è¨€åˆ†å¸ƒç®€å•ç»Ÿè®¡
    korean_comments = len(df[df['comment_text'].str.contains(r'[ã„±-ã…ê°€-í£]', na=False)])
    print(f"ğŸ‡°ğŸ‡· Comments containing Korean: {korean_comments} ({korean_comments/len(df)*100:.1f}%)")
    
    print(f"\nğŸ’¾ Data saved to: {output_filename}")
    print("ğŸ¯ Ready for sentiment analysis!")
    
    # ğŸ” é¢„è§ˆé«˜ç‚¹èµè¯„è®º
    print("\nğŸ”¥ Top 3 Most Liked Comments:")
    top_comments = df.nlargest(3, 'like_count')[['comment_text', 'like_count']]
    for i, row in top_comments.iterrows():
        print(f"{row['like_count']} likes: {row['comment_text'][:100]}...")
