import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# ğŸ¯ åŠ è½½æ¸…æ´—åçš„æ•°æ®
df = pd.read_csv("..\data_collection\clean_comments_ditto_side_a.csv")

print(f"ğŸ“Š Analyzing {len(df)} cleaned comments")
print("=" * 50)

# ================================
# ğŸ’° PHASE 1: RULE-BASED SENTIMENT (FREE)
# ================================

def ditto_specific_sentiment(text):
    """
    é’ˆå¯¹Dittoå’ŒK-popä¼˜åŒ–çš„è§„åˆ™æƒ…æ„Ÿåˆ†æ
    é¿å…Hugging Faceçš„unclearé—®é¢˜
    """
    text_lower = str(text).lower()
    
    # æ­£é¢æƒ…æ„Ÿè¯æ±‡ï¼ˆDittoç‰¹åŒ–ï¼‰
    positive_patterns = {
        'love': ['love', 'amazing', 'perfect', 'beautiful', 'gorgeous', 'talent', 'queen'],
        'nostalgia': ['nostalgia', 'nostalgic', 'memory', 'childhood', 'youth', 'miss'],
        'winter_mood': ['winter', 'cozy', 'warm', 'comfort', 'peaceful', 'calm'],
        'mv_praise': ['genius', 'masterpiece', 'art', 'cinematic', 'story', 'concept'],
        'korean_positive': ['ìµœê³ ', 'ì™„ë²½', 'ì•„ë¦„ë‹¤ì›Œ', 'ì‚¬ë‘', 'ê°ë™']
    }
    
    # è´Ÿé¢æƒ…æ„Ÿè¯æ±‡
    negative_patterns = {
        'sadness': ['sad', 'crying', 'tears', 'heartbreak', 'lonely', 'depressed'],
        'confusion': ['confused', 'weird', 'strange', 'understand', "don't get"],
        'criticism': ['boring', 'bad', 'worst', 'hate', 'terrible'],
        'korean_negative': ['ì‹«ì–´', 'ë³„ë¡œ', 'ì´ìƒí•´', 'ì§€ë£¨í•´']
    }
    
    # ç‰¹æ®Šæƒ…æ„Ÿï¼šDittoç‰¹æœ‰çš„melancholyï¼ˆå¿§éƒä½†ä¸å®Œå…¨è´Ÿé¢ï¼‰
    melancholy_patterns = ['bittersweet', 'melancholy', 'emotional', 'deep', 'thoughtful', 'complex']
    
    pos_score = sum([len(re.findall(word, text_lower)) for category in positive_patterns.values() for word in category])
    neg_score = sum([len(re.findall(word, text_lower)) for category in negative_patterns.values() for word in category])
    mel_score = sum([len(re.findall(word, text_lower)) for word in melancholy_patterns])
    
    # æƒ…æ„Ÿåˆ†ç±»é€»è¾‘
    if mel_score > 0 and pos_score > neg_score:
        return "melancholy_positive"  # Dittoç‰¹æœ‰ï¼šå¿§éƒä½†æ­£é¢
    elif pos_score > neg_score * 1.5:
        return "positive"
    elif neg_score > pos_score * 1.5:
        return "negative"
    elif mel_score > 0:
        return "melancholy"
    else:
        return "neutral"

# åº”ç”¨è§„åˆ™æƒ…æ„Ÿåˆ†æ
print("ğŸ­ Applying rule-based sentiment analysis...")
df['sentiment'] = df['comment_text'].apply(ditto_specific_sentiment)

# æƒ…æ„Ÿåˆ†å¸ƒ
sentiment_dist = df['sentiment'].value_counts()
print("\nğŸ“Š Sentiment Distribution:")
for sentiment, count in sentiment_dist.items():
    percentage = count/len(df)*100
    print(f"   {sentiment}: {count} ({percentage:.1f}%)")

# ================================
# ğŸ” PHASE 2: CLUSTERING ANALYSIS (FREE)
# ================================

print("\nğŸ”® Performing clustering analysis...")

# å‡†å¤‡æ–‡æœ¬æ•°æ®ï¼ˆåªåˆ†æé«˜è´¨é‡è¯„è®ºèŠ‚çœè®¡ç®—ï¼‰
high_quality_df = df[df['quality_score'] > 40].copy()
print(f"ğŸ¯ Analyzing {len(high_quality_df)} high-quality comments for clustering")

# æ–‡æœ¬é¢„å¤„ç†
def preprocess_for_clustering(text):
    # ä¿ç•™è‹±æ–‡ã€éŸ©æ–‡ã€å¸¸è§ç¬¦å·
    text = re.sub(r'[^\w\sê°€-í£]', ' ', str(text).lower())
    # ç§»é™¤è¿‡çŸ­è¯æ±‡
    words = [word for word in text.split() if len(word) > 2]
    return ' '.join(words)

high_quality_df['processed_text'] = high_quality_df['comment_text'].apply(preprocess_for_clustering)

# TF-IDFå‘é‡åŒ–ï¼ˆå¤šè¯­è¨€å‹å¥½ï¼‰
vectorizer = TfidfVectorizer(
    max_features=100,  # æ§åˆ¶ç‰¹å¾æ•°é‡
    min_df=2,          # è¯æ±‡è‡³å°‘å‡ºç°2æ¬¡
    max_df=0.8,        # å¿½ç•¥è¿‡äºå¸¸è§çš„è¯
    ngram_range=(1, 2), # 1-2è¯ç»„åˆ
    stop_words=None    # ä¸ä½¿ç”¨è‹±æ–‡åœç”¨è¯ï¼ˆä¼šåˆ é™¤éŸ©æ–‡ï¼‰
)

# å‘é‡åŒ–
tfidf_matrix = vectorizer.fit_transform(high_quality_df['processed_text'])
feature_names = vectorizer.get_feature_names_out()

# K-meansèšç±»
n_clusters = 5  # åŸºäºä½ çš„comment_typeåˆ†å¸ƒ
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
clusters = kmeans.fit_predict(tfidf_matrix)

high_quality_df['cluster'] = clusters

# ================================
# ğŸ“ˆ PHASE 3: RESULTS VISUALIZATION
# ================================

# 1. æƒ…æ„Ÿ x è¯„è®ºç±»å‹äº¤å‰åˆ†æ
print("\nğŸ“Š Sentiment by Comment Type:")
cross_tab = pd.crosstab(df['comment_type'], df['sentiment'], normalize='index') * 100
print(cross_tab.round(1))

# 2. èšç±»ç»“æœåˆ†æ
print(f"\nğŸ”® Cluster Analysis Results:")
for cluster_id in range(n_clusters):
    cluster_data = high_quality_df[high_quality_df['cluster'] == cluster_id]
    print(f"\nğŸ“ Cluster {cluster_id} ({len(cluster_data)} comments):")
    
    # è·å–èšç±»çš„å…³é”®è¯
    cluster_indices = np.where(clusters == cluster_id)[0]
    cluster_tfidf = tfidf_matrix[cluster_indices].mean(axis=0).A1
    top_features_idx = cluster_tfidf.argsort()[-10:][::-1]
    top_keywords = [feature_names[i] for i in top_features_idx]
    print(f"   Key words: {', '.join(top_keywords[:5])}")
    
    # ä¸»è¦æƒ…æ„Ÿå’Œç±»å‹
    main_sentiment = cluster_data['sentiment'].mode().iloc[0] if len(cluster_data) > 0 else "N/A"
    main_type = cluster_data['comment_type'].mode().iloc[0] if len(cluster_data) > 0 else "N/A"
    print(f"   Main sentiment: {main_sentiment}")
    print(f"   Main type: {main_type}")
    
    # ä»£è¡¨æ€§è¯„è®º
    representative = cluster_data.nlargest(1, 'quality_score')['comment_text'].iloc[0]
    print(f"   Sample: {representative[:100]}...")

# ================================
# ğŸ’ PHASE 4: KEY INSIGHTS EXTRACTION
# ================================

print("\n" + "="*50)
print("ğŸ” KEY INSIGHTS")
print("="*50)

# MV Analysisæƒ…æ„Ÿå€¾å‘
mv_analysis_sentiment = df[df['comment_type'] == 'mv_analysis']['sentiment'].value_counts()
print("ğŸ¬ MV Analysis Comments Sentiment:")
for sentiment, count in mv_analysis_sentiment.head(3).items():
    percentage = count/len(df[df['comment_type'] == 'mv_analysis'])*100
    print(f"   {sentiment}: {percentage:.1f}%")

# è¯­è¨€ä¸æƒ…æ„Ÿå…³ç³»
if 'detected_language' in df.columns:
    print(f"\nğŸŒ Language vs Sentiment Patterns:")
    lang_sentiment = df.groupby('detected_language')['sentiment'].apply(lambda x: x.value_counts(normalize=True).head(2))
    print(lang_sentiment)

# é«˜è´¨é‡è¯„è®ºçš„ç‰¹å¾
high_quality_characteristics = df[df['quality_score'] > 60].groupby('comment_type').size().sort_values(ascending=False)
print(f"\nâ­ High Quality Comments (score>60) by Type:")
for ctype, count in high_quality_characteristics.head(3).items():
    print(f"   {ctype}: {count}")

# ================================
# ğŸ’¡ ACTIONABLE RECOMMENDATIONS
# ================================

print("\nğŸ’¡ RECOMMENDATIONS FOR DEEPER ANALYSIS:")

melancholy_positive_count = len(df[df['sentiment'] == 'melancholy_positive'])
if melancholy_positive_count > 20:
    print(f"ğŸ¯ Focus Area 1: {melancholy_positive_count} 'melancholy_positive' comments - unique Ditto emotion")

mv_analysis_count = len(df[df['comment_type'] == 'mv_analysis'])
if mv_analysis_count > 50:
    print(f"ğŸ¯ Focus Area 2: {mv_analysis_count} MV analysis comments - theory extraction potential")

korean_comments = len(df[df.get('detected_language', '') == 'ko'])
if korean_comments > 100:
    print(f"ğŸ¯ Focus Area 3: {korean_comments} Korean comments - cultural perspective analysis")

print(f"\nğŸ’° Cost-Effective Next Step: Use GPT on top {min(50, mv_analysis_count)} MV analysis comments only")
print("   Expected cost: ~$2-5 for high-value insights")

# ä¿å­˜åˆ†æç»“æœ
df.to_csv("ditto_sentiment_analysis_results.csv", index=False, encoding="utf-8-sig")
print(f"\nğŸ’¾ Results saved to: ditto_sentiment_analysis_results.csv")
