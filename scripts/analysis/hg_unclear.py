import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# è¯»å–æ•°æ®
df = pd.read_csv("labeled_comments_hylt_hf.csv")

print("ğŸ” ANALYZING FAN TYPE CLASSIFICATION ISSUES")
print("="*60)

# 1. åˆ†æunclearçš„è¯„è®ºï¼Œæ‰¾å‡ºæ¨¡å¼
unclear_comments = df[df['fan_type'] == 'unclear']['comment_text'].tolist()
print(f"ğŸ“Š Unclear comments: {len(unclear_comments)} / {len(df)} ({len(unclear_comments)/len(df)*100:.1f}%)")

# ============================================
# ç­–ç•¥1: ä»æ•°æ®ä¸­å‘ç°å…³é”®è¯æ¨¡å¼
# ============================================

def extract_keywords_from_comments(comments, top_n=20):
    """ä»è¯„è®ºä¸­æå–é«˜é¢‘å…³é”®è¯"""
    # æ¸…ç†å’Œåˆ†è¯
    text = ' '.join([str(comment).lower() for comment in comments])
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # ç§»é™¤åœç”¨è¯
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their', 'so', 'very', 'really', 'just', 'too', 'now', 'here', 'there', 'when', 'where', 'how', 'what', 'who', 'which', 'why', 'all', 'some', 'any', 'no', 'not', 'only', 'also', 'even', 'more', 'most', 'much', 'many', 'few', 'little', 'good', 'bad', 'nice', 'great', 'best', 'better', 'well', 'like', 'love', 'know', 'think', 'see', 'get', 'go', 'come', 'want', 'need', 'make', 'take', 'give', 'say', 'tell', 'ask'}
    
    words = text.split()
    filtered_words = [word for word in words if len(word) > 2 and word not in stop_words]
    
    return Counter(filtered_words).most_common(top_n)

# ä¸ºæ¯ä¸ªæˆåŠŸæ ‡æ³¨çš„ç±»å‹æå–å…³é”®è¯
print("\nğŸ” DISCOVERING PATTERNS FROM SUCCESSFULLY LABELED COMMENTS")
print("-" * 60)

for fan_type in ['lyrics-lover', 'visual-lover', 'dance-lover', 'personality-lover', 'music-lover']:
    if fan_type in df['fan_type'].values:
        type_comments = df[df['fan_type'] == fan_type]['comment_text'].tolist()
        if len(type_comments) > 0:
            keywords = extract_keywords_from_comments(type_comments, 15)
            print(f"\nğŸ’ {fan_type.upper()} keywords:")
            print("   ", [f"{word}({count})" for word, count in keywords[:10]])

# ============================================
# ç­–ç•¥2: æ”¹è¿›çš„è§„åˆ™å¼•æ“
# ============================================

def improved_fan_type_classifier(comment):
    """æ”¹è¿›çš„åŸºäºè§„åˆ™çš„åˆ†ç±»å™¨"""
    if pd.isna(comment):
        return "unclear"
    
    text = str(comment).lower()
    
    # æ›´ç²¾ç»†çš„å…³é”®è¯è§„åˆ™
    rules = [
        # Visual-lover è§„åˆ™ (æœ€å®¹æ˜“è¯†åˆ«)
        {
            'type': 'visual-lover',
            'strong_indicators': ['handsome', 'beautiful', 'pretty', 'gorgeous', 'hot', 'sexy', 'cute', 'visual', 'look', 'face', 'hair', 'outfit', 'style', 'fashion', 'makeup', 'å¸…', 'ç¾', 'å¥½çœ‹', 'é¢œå€¼', 'é€ å‹', 'ì™¸ëª¨', 'ì˜ìƒê²¼ë‹¤', 'ã‹ã£ã“ã„ã„', 'ç¾ã—ã„'],
            'weight': 3
        },
        
        # Dance-lover è§„åˆ™
        {
            'type': 'dance-lover', 
            'strong_indicators': ['dance', 'dancing', 'choreography', 'choreo', 'moves', 'move', 'performance', 'perform', 'stage', 'rhythm', 'beat', 'èˆè¹ˆ', 'ç¼–èˆ', 'åŠ¨ä½œ', 'è¡¨æ¼”', 'ëŒ„ìŠ¤', 'ì•ˆë¬´', 'ãƒ€ãƒ³ã‚¹', 'è¸Šã‚Š'],
            'weight': 3
        },
        
        # Music-lover è§„åˆ™  
        {
            'type': 'music-lover',
            'strong_indicators': ['beat', 'melody', 'music', 'sound', 'production', 'instrumental', 'vocals', 'voice', 'singing', 'audio', 'quality', 'mix', 'éŸ³ä¹', 'ç¼–æ›²', 'æ—‹å¾‹', 'å£°éŸ³', 'ìŒì•…', 'ë©œë¡œë””', 'éŸ³æ¥½', 'ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼'],
            'weight': 3
        },
        
        # Lyrics-lover è§„åˆ™
        {
            'type': 'lyrics-lover',
            'strong_indicators': ['lyrics', 'words', 'meaning', 'message', 'deep', 'story', 'relate', 'understand', 'poetry', 'verse', 'line', 'æ­Œè¯', 'æ„æ€', 'å«ä¹‰', 'æ•…äº‹', 'ê°€ì‚¬', 'ì˜ë¯¸', 'æ­Œè©', 'è¨€è‘‰'],
            'weight': 3
        },
        
        # Personality-lover è§„åˆ™ (æœ€éš¾è¯†åˆ«ï¼Œç”¨ç»„åˆè§„åˆ™)
        {
            'type': 'personality-lover',
            'strong_indicators': ['funny', 'humor', 'sweet', 'kind', 'real', 'genuine', 'humble', 'personality', 'character', 'person', 'talented', 'skill', 'æ€§æ ¼', 'äººå“', 'æç¬‘', 'å–„è‰¯', 'çœŸå®', 'ì„±ê²©', 'ì¸ì„±', 'é¢ç™½ã„', 'å„ªã—ã„'],
            'weight': 2,
            'combo_rules': [
                # ç»„åˆè§„åˆ™ï¼šæåˆ°äºº+æ­£é¢è¯æ±‡
                (['person', 'people', 'human', 'individual'], ['amazing', 'incredible', 'perfect', 'awesome', 'wonderful']),
                # æåˆ°æ‰èƒ½+ä¸ªäººç‰¹è´¨
                (['talent', 'skill', 'ability', 'gift'], ['natural', 'born', 'amazing', 'incredible'])
            ]
        }
    ]
    
    # è®¡ç®—æ¯ç§ç±»å‹çš„å¾—åˆ†
    type_scores = defaultdict(float)
    
    for rule in rules:
        fan_type = rule['type']
        weight = rule['weight']
        
        # åŸºç¡€å…³é”®è¯å¾—åˆ†
        keyword_score = 0
        for keyword in rule['strong_indicators']:
            if keyword in text:
                keyword_score += 1
        
        # ç»„åˆè§„åˆ™å¾—åˆ† (å¦‚æœæœ‰)
        combo_score = 0
        if 'combo_rules' in rule:
            for combo in rule['combo_rules']:
                group1, group2 = combo
                has_group1 = any(word in text for word in group1)
                has_group2 = any(word in text for word in group2)
                if has_group1 and has_group2:
                    combo_score += 2
        
        type_scores[fan_type] = (keyword_score + combo_score) * weight
    
    # ç‰¹æ®Šè§„åˆ™ï¼šé•¿åº¦è¿‡çŸ­çš„è¯„è®º
    if len(text.strip()) < 10:
        return "unclear"
    
    # ç‰¹æ®Šè§„åˆ™ï¼šåªæœ‰è¡¨æƒ…ç¬¦å·æˆ–é‡å¤å­—ç¬¦
    if re.match(r'^[^\w\s]*$', text) or len(set(text.replace(' ', ''))) < 3:
        return "unclear"
    
    # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
    if type_scores and max(type_scores.values()) > 0:
        return max(type_scores, key=type_scores.get)
    else:
        return "unclear"

# é‡æ–°åˆ†ç±»unclearçš„è¯„è®º
print("\nğŸ”§ RE-CLASSIFYING UNCLEAR COMMENTS WITH IMPROVED RULES")
print("-" * 60)

unclear_mask = df['fan_type'] == 'unclear'
unclear_indices = df[unclear_mask].index

reclassified_count = 0
for idx in unclear_indices:
    comment = df.loc[idx, 'comment_text']
    new_type = improved_fan_type_classifier(comment)
    if new_type != 'unclear':
        df.loc[idx, 'fan_type'] = new_type
        reclassified_count += 1

print(f"ğŸ“Š Reclassified {reclassified_count} comments from unclear to specific types")

# æ–°çš„åˆ†å¸ƒç»Ÿè®¡
new_fan_type_counts = df["fan_type"].value_counts()
new_unclear_ratio = new_fan_type_counts.get('unclear', 0) / len(df) * 100

print(f"\nğŸ“ˆ IMPROVED RESULTS:")
print(f"   â€¢ Unclear ratio: {len(unclear_comments)/len(df)*100:.1f}% â†’ {new_unclear_ratio:.1f}%")
print(f"   â€¢ Improvement: {len(unclear_comments)/len(df)*100 - new_unclear_ratio:.1f}% reduction in unclear")

print(f"\nğŸ†• NEW FAN TYPE DISTRIBUTION:")
for fan_type, count in new_fan_type_counts.items():
    percentage = count/len(df)*100
    print(f"   {fan_type}: {count} ({percentage:.1f}%)")

# ============================================
# ç­–ç•¥3: æ–‡æœ¬èšç±»å‘ç°æœªçŸ¥æ¨¡å¼
# ============================================

print("\nğŸ§  DISCOVERING HIDDEN PATTERNS WITH TEXT CLUSTERING")
print("-" * 60)

# å¯¹remaining unclearè¯„è®ºè¿›è¡Œèšç±»åˆ†æ
remaining_unclear = df[df['fan_type'] == 'unclear']['comment_text'].tolist()

if len(remaining_unclear) > 10:
    # TF-IDFå‘é‡åŒ–
    vectorizer = TfidfVectorizer(
        max_features=100, 
        stop_words='english',
        ngram_range=(1, 2),
        min_df=2
    )
    
    try:
        # è¿‡æ»¤ç©ºè¯„è®º
        valid_comments = [str(comment) for comment in remaining_unclear if str(comment).strip() and len(str(comment)) > 5]
        
        if len(valid_comments) > 5:
            tfidf_matrix = vectorizer.fit_transform(valid_comments)
            
            # K-meansèšç±»
            n_clusters = min(5, len(valid_comments) // 3)  # åŠ¨æ€ç¡®å®šèšç±»æ•°
            if n_clusters >= 2:
                kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                clusters = kmeans.fit_predict(tfidf_matrix)
                
                # åˆ†ææ¯ä¸ªèšç±»çš„ç‰¹å¾è¯
                feature_names = vectorizer.get_feature_names_out()
                
                print(f"ğŸ” Found {n_clusters} hidden patterns in unclear comments:")
                
                for i in range(n_clusters):
                    cluster_comments = [valid_comments[j] for j in range(len(valid_comments)) if clusters[j] == i]
                    cluster_size = len(cluster_comments)
                    
                    if cluster_size > 1:
                        # è·å–è¯¥èšç±»çš„ä»£è¡¨æ€§è¯æ±‡
                        cluster_indices = np.where(clusters == i)[0]
                        cluster_center = kmeans.cluster_centers_[i]
                        top_features = cluster_center.argsort()[-10:][::-1]
                        top_words = [feature_names[idx] for idx in top_features]
                        
                        print(f"\n   Cluster {i+1} ({cluster_size} comments):")
                        print(f"   Key words: {', '.join(top_words[:5])}")
                        print(f"   Sample: {cluster_comments[0][:60]}...")
                        
                        # åŸºäºå…³é”®è¯å»ºè®®å¯èƒ½çš„ç±»å‹
                        if any(word in ' '.join(top_words) for word in ['love', 'heart', 'feel', 'emotion']):
                            print(f"   ğŸ’¡ Suggests: Might be personality-lover or emotional response")
                        elif any(word in ' '.join(top_words) for word in ['song', 'music', 'sound']):
                            print(f"   ğŸ’¡ Suggests: Might be music-lover")
    
    except Exception as e:
        print(f"âŒ Clustering analysis failed: {e}")

# ============================================
# ç­–ç•¥4: æ‰‹å·¥è§„åˆ™ä¼˜åŒ–å»ºè®®
# ============================================

print(f"\nğŸ’¡ MANUAL INSPECTION SUGGESTIONS:")
print("-" * 60)

# æ˜¾ç¤ºä¸€äº›unclearæ ·æœ¬ä¾›æ‰‹å·¥æ£€æŸ¥
unclear_samples = df[df['fan_type'] == 'unclear'].head(10)
print(f"\nğŸ“ Sample unclear comments for manual review:")

for idx, row in unclear_samples.iterrows():
    comment = str(row['comment_text'])[:80]
    emotion = row['emotion']
    print(f"\n   Comment: {comment}...")
    print(f"   Emotion: {emotion}")
    print(f"   ğŸ’­ Manual suggestion: _____ (ä½ å¯ä»¥æ‰‹å·¥åˆ¤æ–­)")

# ä¿å­˜æ”¹è¿›åçš„ç»“æœ
df.to_csv("labeled_comments_hylt_improved.csv", index=False, encoding="utf-8-sig")

print(f"\nğŸ’¾ SAVED IMPROVED RESULTS:")
print(f"   âœ… labeled_comments_hylt_improved.csv")

print(f"\nğŸ¯ RECOMMENDATIONS FOR NEXT ITERATION:")
print(f"   1. ğŸ“ Manually review top unclear samples to build better rules")
print(f"   2. ğŸ”„ Add discovered keywords to the classifier") 
print(f"   3. ğŸ“Š Test on comments from different K-pop songs")
print(f"   4. ğŸ¤– Consider using a different approach for personality-lover detection")
print(f"   5. ğŸ“ˆ Focus on the classifications that ARE working well")

# ç”Ÿæˆæ”¹è¿›å»ºè®®æŠ¥å‘Š
print(f"\nğŸ“‹ CLASSIFICATION PERFORMANCE REPORT:")
print(f"   â€¢ Best performing types: {new_fan_type_counts.head(3).to_dict()}")
print(f"   â€¢ Most challenging type: Likely 'personality-lover' (too subjective)")
print(f"   â€¢ Recommended focus: Visual, Dance, Music (easier to identify)")